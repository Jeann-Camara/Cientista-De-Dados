{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import o findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame, Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # biblioteca de visualização utilizada pelo pandas e pelo seaborn\n",
    "import seaborn as sns # biblioteca de visualização com mais opções de gráficos\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Criando uma Sessão Spark. Na sessão é possível configurar os nós do cluster, bem como a memória alocada para cada um deles, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark Session**: Fornece um único ponto de entrada para interagir com as funcionalidades fundamentais do Spark e permite programar o Spark com o Dataframe e outras APIs de Conjunto de Dados, como HIVE, Streaming e API Sql . Toda a funcionalidade disponível com o sparkContext também está disponível no sparkSession. Para usar APIs de SQL, HIVE e Streaming, não é necessário criar contextos separados, pois o sparkSession inclui todas as APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Learn pyspark\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark Context**: é usado como um canal para acessar as funcionalidade do spark.  O programa spark driver usa o contexto spark para conectar-se ao cluster através de  um gerenciador de recursos (YARN ouMesos ..)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lendo um arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_TRAIN_PATH = 'titanic_train.csv'\n",
    "FILE_TEST_PATH = 'titanic_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(FILE_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked',\n",
       " '1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2) # Exibir os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['PassengerId',\n",
       "  'Survived',\n",
       "  'Pclass',\n",
       "  'Name',\n",
       "  'Sex',\n",
       "  'Age',\n",
       "  'SibSp',\n",
       "  'Parch',\n",
       "  'Ticket',\n",
       "  'Fare',\n",
       "  'Cabin',\n",
       "  'Embarked'],\n",
       " ['1',\n",
       "  '0',\n",
       "  '3',\n",
       "  '\"Braund',\n",
       "  ' Mr. Owen Harris\"',\n",
       "  'male',\n",
       "  '22',\n",
       "  '1',\n",
       "  '0',\n",
       "  'A/5 21171',\n",
       "  '7.25',\n",
       "  '',\n",
       "  'S']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ler os dados e aplicar o split dos dados\n",
    "rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar o primeiro elemento\n",
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um rdd e definindo a estrutura da Row\n",
    "rdd_to_df = rdd.map(lambda line: Row(PassengerId=line[0], Survived=line[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rdd_to_df.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|PassengerId|Survived|\n",
      "+-----------+--------+\n",
      "|PassengerId|Survived|\n",
      "|          1|       0|\n",
      "|          2|       1|\n",
      "|          3|       1|\n",
      "|          4|       1|\n",
      "+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um rdd, aplicando filtro e definindo a estrutura da Row\n",
    "rdd = sc.textFile(FILE_TRAIN_PATH) \\\n",
    "    .map(lambda line: line.split(\",\")) \\\n",
    "    .filter(lambda line: len(line)>1) \\\n",
    "    .map(lambda line: Row(PassengerId= line[0], Survived= line[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo o header do arquivo CSV\n",
    "new_rdd = rdd.subtract(sc.parallelize([rdd.first()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(PassengerId='PassengerId', Survived='Survived'),\n",
       " Row(PassengerId='1', Survived='0')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(PassengerId='1', Survived='0'), Row(PassengerId='4', Survived='1')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O Famoso count words através da operação map reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = sc.textFile(FILE_TRAIN_PATH) \\\n",
    "        .flatMap(lambda line: line.split(\",\")) \\\n",
    "        .map(lambda word: (word, 1)) \\\n",
    "        .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PassengerId', 1),\n",
       " ('Survived', 1),\n",
       " ('Pclass', 1),\n",
       " ('Name', 1),\n",
       " ('Sex', 1),\n",
       " ('Age', 1),\n",
       " ('SibSp', 1),\n",
       " ('Parch', 1),\n",
       " ('Ticket', 1),\n",
       " ('Fare', 1),\n",
       " ('Cabin', 1),\n",
       " ('Embarked', 1),\n",
       " ('1', 893),\n",
       " ('0', 1850),\n",
       " ('3', 519),\n",
       " ('\"Braund', 2),\n",
       " (' Mr. Owen Harris\"', 1),\n",
       " ('male', 577),\n",
       " ('22', 28),\n",
       " ('A/5 21171', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = spark.read.csv(FILE_TRAIN_PATH, header=True, inferSchema=True) #inferSchema=False, schema=schema\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(FILE_TRAIN_PATH)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A partir de um data frame pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"PassengerId\", IntegerType(), True),\n",
    "                     StructField(\"Survived\", IntegerType(), True),\n",
    "                     StructField(\"Pclass\", IntegerType(), True),\n",
    "                     StructField(\"Name\", StringType(), True),\n",
    "                     StructField(\"Sex\", StringType(), True),\n",
    "                     StructField(\"Age\", DoubleType(), True),\n",
    "                     StructField(\"SibSp\", IntegerType(), True),\n",
    "                     StructField(\"Parch\", IntegerType(), True),\n",
    "                     StructField(\"Ticket\", StringType(), True),\n",
    "                     StructField(\"Fare\", DoubleType(), True),\n",
    "                     StructField(\"Cabin\", StringType(), True),\n",
    "                     StructField(\"Embarked\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = spark.read.csv(FILE_TRAIN_PATH, header=True, inferSchema=False, schema=schema) #inferSchema=False, schema=schema\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(FILE_TRAIN_PATH)\n",
    "data = spark.createDataFrame(df, schema=schema)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A partir de um rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: string, Survived: string]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_aux = spark.createDataFrame(new_rdd) #df = spark.createDataFrame(rdd, [columns])\n",
    "data_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A partir de um arquivo parquet do HDFS ... sqlContext.read.parquet(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando as dados e informações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(PassengerId=1, Survived=0, Pclass=3, Name='Braund, Mr. Owen Harris', Sex='male', Age=22.0, SibSp=1, Parch=0, Ticket='A/5 21171', Fare=7.25, Cabin='NaN', Embarked='S'),\n",
       " Row(PassengerId=2, Survived=1, Pclass=1, Name='Cumings, Mrs. John Bradley (Florence Briggs Thayer)', Sex='female', Age=38.0, SibSp=1, Parch=0, Ticket='PC 17599', Fare=71.2833, Cabin='C85', Embarked='C')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|   Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|A/5 21171|   7.25|  NaN|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0| PC 17599|71.2833|  C85|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(2) # Printa as n primeira linhas no console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+------+----+-----------------+\n",
      "|summary|           Survived|            Pclass|   Sex| Age|             Fare|\n",
      "+-------+-------------------+------------------+------+----+-----------------+\n",
      "|  count|                891|               891|   891| 891|              891|\n",
      "|   mean| 0.3838383838383838| 2.308641975308642|  null| NaN| 32.2042079685746|\n",
      "| stddev|0.48659245426485753|0.8360712409770491|  null| NaN|49.69342859718089|\n",
      "|    min|                  0|                 1|female|0.42|              0.0|\n",
      "|    max|                  1|                 3|  male| NaN|         512.3292|\n",
      "+-------+-------------------+------------------+------+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calcula estatísticas básicas para colunas numéricas e nominais.\n",
    "data.select('Survived','Pclass','Sex','Age', 'Fare').describe().show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+------+----+-----------------+\n",
      "|summary|           Survived|            Pclass|   Sex| Age|             Fare|\n",
      "+-------+-------------------+------------------+------+----+-----------------+\n",
      "|  count|                891|               891|   891| 891|              891|\n",
      "|   mean| 0.3838383838383838| 2.308641975308642|  null| NaN| 32.2042079685746|\n",
      "| stddev|0.48659245426485753|0.8360712409770491|  null| NaN|49.69342859718089|\n",
      "|    min|                  0|                 1|female|0.42|              0.0|\n",
      "|    25%|                  0|                 2|  null|22.0|           7.8958|\n",
      "|    50%|                  0|                 3|  null|32.0|          14.4542|\n",
      "|    75%|                  1|                 3|  null|54.0|             31.0|\n",
      "|    max|                  1|                 3|  male| NaN|         512.3292|\n",
      "+-------+-------------------+------------------+------+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calcula mais informações estatísticas para colunas numéricas e nominais.\n",
    "data.select('Survived','Pclass','Sex','Age', 'Fare').summary().show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removendo dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+------+----+-----------------+\n",
      "|summary|           Survived|            Pclass|   Sex| Age|             Fare|\n",
      "+-------+-------------------+------------------+------+----+-----------------+\n",
      "|  count|                891|               891|   891| 891|              891|\n",
      "|   mean| 0.3838383838383838| 2.308641975308642|  null| NaN| 32.2042079685746|\n",
      "| stddev|0.48659245426485753|0.8360712409770491|  null| NaN|49.69342859718089|\n",
      "|    min|                  0|                 1|female|0.42|              0.0|\n",
      "|    25%|                  0|                 2|  null|22.0|           7.8958|\n",
      "|    50%|                  0|                 3|  null|32.0|          14.4542|\n",
      "|    75%|                  1|                 3|  null|54.0|             31.0|\n",
      "|    max|                  1|                 3|  male| NaN|         512.3292|\n",
      "+-------+-------------------+------------------+------+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('Survived','Pclass','Sex','Age', 'Fare').summary().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------+------------------+-----------------+\n",
      "|summary|          Survived|            Pclass|   Sex|               Age|             Fare|\n",
      "+-------+------------------+------------------+------+------------------+-----------------+\n",
      "|  count|               714|               714|   714|               714|              714|\n",
      "|   mean|0.4061624649859944|2.2366946778711485|  null| 29.69911764705882|34.69451400560218|\n",
      "| stddev|0.4914598643353704| 0.838249862698379|  null|14.526497332334035|52.91892950254356|\n",
      "|    min|                 0|                 1|female|              0.42|              0.0|\n",
      "|    25%|                 0|                 1|  null|              20.0|             8.05|\n",
      "|    50%|                 0|                 2|  null|              28.0|          15.7417|\n",
      "|    75%|                 1|                 3|  null|              38.0|             33.5|\n",
      "|    max|                 1|                 3|  male|              80.0|         512.3292|\n",
      "+-------+------------------+------------------+------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('Survived','Pclass','Sex','Age', 'Fare').summary().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "| Age|   Sex|\n",
      "+----+------+\n",
      "|49.0|female|\n",
      "|55.0|female|\n",
      "|47.0|  male|\n",
      "|0.83|  male|\n",
      "|12.0|  male|\n",
      "|66.0|  male|\n",
      "|64.0|  male|\n",
      "|18.0|female|\n",
      "|36.0|  male|\n",
      "|44.0|  male|\n",
      "+----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('Age','Sex').dropDuplicates().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Realizando input de todos os valores nulos pelo valor passado como parametro\n",
    "data.fillna(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25|  NaN|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925|  NaN|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05|  NaN|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Realizando input de dados apenas para as colunas especificadas\n",
    "data.na.fill({'Age': 29, 'Name': 'Unknown'}).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Name|\n",
      "+--------------------+\n",
      "|Braund, Mr. Owen ...|\n",
      "|Cumings, Mrs. Joh...|\n",
      "|Heikkinen, Miss. ...|\n",
      "|Futrelle, Mrs. Ja...|\n",
      "|Allen, Mr. Willia...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('Name').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|                Name| Age|\n",
      "+--------------------+----+\n",
      "|Braund, Mr. Owen ...|22.0|\n",
      "|Cumings, Mrs. Joh...|38.0|\n",
      "|Heikkinen, Miss. ...|26.0|\n",
      "|Futrelle, Mrs. Ja...|35.0|\n",
      "|Allen, Mr. Willia...|35.0|\n",
      "+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('Name','Age').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Braund, Mr. Owen Harris', Age=22.0),\n",
       " Row(Name='Cumings, Mrs. John Bradley (Florence Briggs Thayer)', Age=38.0),\n",
       " Row(Name='Heikkinen, Miss. Laina', Age=26.0),\n",
       " Row(Name='Futrelle, Mrs. Jacques Heath (Lily May Peel)', Age=35.0),\n",
       " Row(Name='Allen, Mr. William Henry', Age=35.0)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select('Name','Age').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|New_Age|\n",
      "+-------+\n",
      "|   44.0|\n",
      "|   76.0|\n",
      "|   52.0|\n",
      "|   70.0|\n",
      "|   70.0|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.selectExpr('(Age * 2) as New_Age').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select('Age').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+-------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch| Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+-------+-------+-----+--------+\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1| 349909| 21.075|  NaN|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0| 237736|30.0708|  NaN|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|PP 9549|   16.7|   G6|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0| 350406| 7.8542|  NaN|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1| 382652| 29.125|  NaN|       Q|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+-------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.filter(data.Age < 18).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+-------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch| Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+-------+-------+-----+--------+\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1| 349909| 21.075|  NaN|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0| 237736|30.0708|  NaN|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|PP 9549|   16.7|   G6|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0| 350406| 7.8542|  NaN|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1| 382652| 29.125|  NaN|       Q|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+-------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.filter('Age < 18').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+-------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch| Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+-------+-------+-----+--------+\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1| 349909| 21.075|  NaN|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0| 237736|30.0708|  NaN|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|PP 9549|   16.7|   G6|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0| 350406| 7.8542|  NaN|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1| 382652| 29.125|  NaN|       Q|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+-------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.where('Age < 18').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+----+----+-----+-----+--------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name| Sex| Age|SibSp|Parch|  Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+----+----+-----+-----+--------+-------+-----+--------+\n",
      "|        631|       1|     1|Barkworth, Mr. Al...|male|80.0|    0|    0|   27042|   30.0|  A23|       S|\n",
      "|        852|       0|     3| Svensson, Mr. Johan|male|74.0|    0|    0|  347060|  7.775|  NaN|       S|\n",
      "|        494|       0|     1|Artagaveytia, Mr....|male|71.0|    0|    0|PC 17609|49.5042|  NaN|       C|\n",
      "|         97|       0|     1|Goldschmidt, Mr. ...|male|71.0|    0|    0|PC 17754|34.6542|   A5|       C|\n",
      "|        117|       0|     3|Connors, Mr. Patrick|male|70.5|    0|    0|  370369|   7.75|  NaN|       Q|\n",
      "+-----------+--------+------+--------------------+----+----+-----+-----+--------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.orderBy(data.Age.desc()).show(5) # Listar primeiro os mais idosos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+----+----+-----+-----+--------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name| Sex| Age|SibSp|Parch|  Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+----+----+-----+-----+--------+-------+-----+--------+\n",
      "|        631|       1|     1|Barkworth, Mr. Al...|male|80.0|    0|    0|   27042|   30.0|  A23|       S|\n",
      "|        852|       0|     3| Svensson, Mr. Johan|male|74.0|    0|    0|  347060|  7.775|  NaN|       S|\n",
      "|        494|       0|     1|Artagaveytia, Mr....|male|71.0|    0|    0|PC 17609|49.5042|  NaN|       C|\n",
      "|         97|       0|     1|Goldschmidt, Mr. ...|male|71.0|    0|    0|PC 17754|34.6542|   A5|       C|\n",
      "|        117|       0|     3|Connors, Mr. Patrick|male|70.5|    0|    0|  370369|   7.75|  NaN|       Q|\n",
      "+-----------+--------+------+--------------------+----+----+-----+-----+--------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.sort('Age', ascending=False).show(5) # Listar primeiro os mais idosos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subtract = data.select('PassengerId') \\\n",
    "                .subtract(data.filter(data.PassengerId == 1).select('PassengerId'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|PassengerId|\n",
      "+-----------+\n",
      "|          2|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_subtract.filter('PassengerId in (1,2)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------------+\n",
      "|PassengerId|   Fare|new_category|\n",
      "+-----------+-------+------------+\n",
      "|          1|   7.25|     cheaper|\n",
      "|          2|71.2833|   expensive|\n",
      "|          3|  7.925|     cheaper|\n",
      "|          4|   53.1|   expensive|\n",
      "|          5|   8.05|     cheaper|\n",
      "+-----------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(data.PassengerId, data.Fare,\n",
    "          F.when(data.Fare <= 50, 'cheaper') \\\n",
    "          .otherwise('expensive').alias('new_category')\n",
    "           ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------------+\n",
      "|PassengerId|   Fare|new_category|\n",
      "+-----------+-------+------------+\n",
      "|          1|   7.25|     cheaper|\n",
      "|          2|71.2833|   cheaper_2|\n",
      "|          3|  7.925|     cheaper|\n",
      "|          4|   53.1|   cheaper_2|\n",
      "|          5|   8.05|     cheaper|\n",
      "+-----------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(data.PassengerId, data.Fare,\n",
    "          F.when(data.Fare <= 50, 'cheaper') \\\n",
    "            .when((data.Fare > 50) & (data.Fare < 100), 'cheaper_2') \\\n",
    "            .otherwise('expensive').alias('new_category')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realizando join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data = spark.read.csv('titanic_train_fake.csv', header=True, inferSchema=True) #inferSchema=False, schema=schema\n",
    "fake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`Other_Info`' given input columns: [Pclass, Survived, Name, Parch, PassengerId, Parch, Fare, Sex, Name, Ticket, Ticket, Cabin, SibSp, Age, SibSp, Embarked, Age, PassengerId, Pclass, Sex, Embarked, Cabin, Fare];;\\n'Project [PassengerId#2247 AS PI#2338, PassengerId#104 AS DataPassengerId#2339, Survived#105, 'Other_Info]\\n+- AnalysisBarrier\\n      +- Join Inner, (PassengerId#104 = PassengerId#2247)\\n         :- Filter AtLeastNNulls(n, PassengerId#104,Survived#105,Pclass#106,Name#107,Sex#108,Age#109,SibSp#110,Parch#111,Ticket#112,Fare#113,Cabin#114,Embarked#115)\\n         :  +- LogicalRDD [PassengerId#104, Survived#105, Pclass#106, Name#107, Sex#108, Age#109, SibSp#110, Parch#111, Ticket#112, Fare#113, Cabin#114, Embarked#115], false\\n         +- Relation[PassengerId#2247,Pclass#2248,Name#2249,Sex#2250,Age#2251,SibSp#2252,Parch#2253,Ticket#2254,Fare#2255,Cabin#2256,Embarked#2257] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.3.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    319\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o483.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`Other_Info`' given input columns: [Pclass, Survived, Name, Parch, PassengerId, Parch, Fare, Sex, Name, Ticket, Ticket, Cabin, SibSp, Age, SibSp, Embarked, Age, PassengerId, Pclass, Sex, Embarked, Cabin, Fare];;\n'Project [PassengerId#2247 AS PI#2338, PassengerId#104 AS DataPassengerId#2339, Survived#105, 'Other_Info]\n+- AnalysisBarrier\n      +- Join Inner, (PassengerId#104 = PassengerId#2247)\n         :- Filter AtLeastNNulls(n, PassengerId#104,Survived#105,Pclass#106,Name#107,Sex#108,Age#109,SibSp#110,Parch#111,Ticket#112,Fare#113,Cabin#114,Embarked#115)\n         :  +- LogicalRDD [PassengerId#104, Survived#105, Pclass#106, Name#107, Sex#108, Age#109, SibSp#110, Parch#111, Ticket#112, Fare#113, Cabin#114, Embarked#115], false\n         +- Relation[PassengerId#2247,Pclass#2248,Name#2249,Sex#2250,Age#2251,SibSp#2252,Parch#2253,Ticket#2254,Fare#2255,Cabin#2256,Embarked#2257] csv\n\r\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:120)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:120)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:125)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:125)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:80)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:80)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:104)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3295)\r\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1307)\r\n\tat sun.reflect.GeneratedMethodAccessor67.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-a953cc2db653>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPassengerId\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mfake_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPassengerId\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPassengerId\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PI'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPassengerId\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataPassengerId'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Survived'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Other_Info'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   1200\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \"\"\"\n\u001b[1;32m-> 1202\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1203\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.3.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1160\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"cannot resolve '`Other_Info`' given input columns: [Pclass, Survived, Name, Parch, PassengerId, Parch, Fare, Sex, Name, Ticket, Ticket, Cabin, SibSp, Age, SibSp, Embarked, Age, PassengerId, Pclass, Sex, Embarked, Cabin, Fare];;\\n'Project [PassengerId#2247 AS PI#2338, PassengerId#104 AS DataPassengerId#2339, Survived#105, 'Other_Info]\\n+- AnalysisBarrier\\n      +- Join Inner, (PassengerId#104 = PassengerId#2247)\\n         :- Filter AtLeastNNulls(n, PassengerId#104,Survived#105,Pclass#106,Name#107,Sex#108,Age#109,SibSp#110,Parch#111,Ticket#112,Fare#113,Cabin#114,Embarked#115)\\n         :  +- LogicalRDD [PassengerId#104, Survived#105, Pclass#106, Name#107, Sex#108, Age#109, SibSp#110, Parch#111, Ticket#112, Fare#113, Cabin#114, Embarked#115], false\\n         +- Relation[PassengerId#2247,Pclass#2248,Name#2249,Sex#2250,Age#2251,SibSp#2252,Parch#2253,Ticket#2254,Fare#2255,Cabin#2256,Embarked#2257] csv\\n\""
     ]
    }
   ],
   "source": [
    "data.join(fake_data, data.PassengerId == fake_data.PassengerId)\\\n",
    "    .select(fake_data.PassengerId.alias('PI'), data.PassengerId.alias('DataPassengerId'), 'Survived', 'Other_Info').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|PassengerId|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fake_data = fake_data.alias('dado_fake')\n",
    "data = data.alias('data')\n",
    "data.join(fake_data, F.col(\"dado_fake.PassengerId\") == F.col(\"data.PassengerId\"), 'inner').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrupar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupBy(\"Pclass\") \\\n",
    ".agg(F.sum('Fare').alias('soma'), F.mean('Fare').alias('media'),  F.count(\"*\")) \\\n",
    ".show(5) ### Agrupar os dados por um determinado pivô."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupBy(\"Pclass\") \\\n",
    "    .agg(F.sum('Fare').alias(\"soma\")) \\\n",
    "    .sort('soma', descending=True).show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use crosstab operation on DataFrame to calculate the pair wise frequency of columns. \n",
    "data.crosstab('Pclass', 'Sex').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando uma nova coluna novos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.withColumn('New_Fare', (data['Fare'] + 1.5).cast(FloatType())).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Através de udfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cheaper(fare, base_fare):\n",
    "    return fare>base_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.withColumn('Is_Cheaper', is_cheaper(data['Fare'], 50).cast(BooleanType())).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cheaper_udf = F.udf(lambda fare, base_fare: fare>base_fare, BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.withColumn('Is_Cheaper', is_cheaper_udf(data['Fare'], F.lit(50))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizar colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removendo colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('SibSp', 'Ticket').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('SibSp').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renomeando colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.withColumnRenamed('Sex', 'Gender').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mais operações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select('Age', 'Sex').rdd.map(lambda x:(x[0], x[1], 1)).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Queries on DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_TABLE = 'train_temp_table'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView(TEMP_TABLE) ## Temp table createOrReplaceTempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from {temp_table}'.format(temp_table=TEMP_TABLE)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select Age, max(Fare) from {temp_table} group by Age order by Age desc'.format(temp_table=TEMP_TABLE)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converter para dataFrame pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPandasDF = data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataPandasDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPandasDF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalizando o Spark Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.cetax.com.br/blog/tutorial-pyspark-e-mllib/\n",
    "- http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html\n",
    "- https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/\n",
    "- http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "\n",
    "<!--\n",
    "# Import 'DenseVector'\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "##### Define the 'input_data'\n",
    "input_data = data.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "#### Import 'DenseVector'\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "#### Define the 'input_data'\n",
    "input_data = data.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "#### Replace 'df' with the new DataFrame\n",
    "df = spark.createDataFrame(input_data, [\"label\", \"features\"])\n",
    "\n",
    "#=========================================================================================================\n",
    "#### Import \"StandardScaler\"\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "#### Initialize the \"standardScaler\"\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "#### Fit the DataFrame to the scaler\n",
    "scaler = standardScaler.fit(df)\n",
    "\n",
    "#### Transform the data in 'df' with the scaler\n",
    "scaled_df = scaler.transform(df)\n",
    "\n",
    "#### Inspect the result\n",
    "scaled_df.show(2)\n",
    "\n",
    "#=========================================================================================================\n",
    "\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=3, inputCol=\"features_scaled\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(scaled_df)\n",
    "\n",
    "result = model.transform(scaled_df).select(\"pcaFeatures\")\n",
    "result.show(truncate=False) Import 'DenseVector'\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "#### Define the 'input_data'\n",
    "input_data = data.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "#### Replace 'df' with the new DataFrame\n",
    "df = spark.createDataFrame(input_data, [\"label\", \"features\"])\n",
    "\n",
    "#=========================================================================================================\n",
    "#### Import \"StandardScaler\"\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "#### Initialize the \"standardScaler\"\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "#### Fit the DataFrame to the scaler\n",
    "scaler = standardScaler.fit(df)\n",
    "\n",
    "#### Transform the data in 'df' with the scaler\n",
    "scaled_df = scaler.transform(df)\n",
    "\n",
    "#### Inspect the result\n",
    "scaled_df.show(2)\n",
    "\n",
    "#=========================================================================================================\n",
    "\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=3, inputCol=\"features_scaled\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(scaled_df)\n",
    "\n",
    "result = model.transform(scaled_df).select(\"pcaFeatures\")\n",
    "result.show(truncate=False) Replace 'df' with the new DataFrame\n",
    "df = spark.createDataFrame(input_data, [\"label\", \"features\"])\n",
    "\n",
    "#=========================================================================================================\n",
    "#### Import \"StandardScaler\"\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "#### Initialize the \"standardScaler\"\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "#### Fit the DataFrame to the scaler\n",
    "scaler = standardScaler.fit(df)\n",
    "\n",
    "#### Transform the data in 'df' with the scaler\n",
    "scaled_df = scaler.transform(df)\n",
    "\n",
    "#### Inspect the result\n",
    "scaled_df.show(2)\n",
    "\n",
    "#=========================================================================================================\n",
    "\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=3, inputCol=\"features_scaled\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(scaled_df)\n",
    "\n",
    "result = model.transform(scaled_df).select(\"pcaFeatures\")\n",
    "result.show(truncate=False)\n",
    "\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
